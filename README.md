# Crochet-MultiModal-LLMs
A multimodal LLM Research Repository.
Contributors : Hariansh Vashist | Divyansh goyal


Research Statement : 
### Hallucinations in Multimodal Large Language Models based on vectorsearch datasets. || Improving OCR capabilities in Large Vision Language Models (LVLM's)
the latter is what is our focus is on at the moment.
### Currently Exploring : Google Gemini Pro + DOCVQA 


Key terminologies:
### Ground truth in datasets 
### Multimodality
### Evaluation metric 

Compute:
### Azure machine learning studio (free credit accounts exist but they will require student ID's)

References : 
[https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models]Awesome MultiModal LLM's Large Language Models(github repo)
[https://huyenchip.com/2023/10/10/multimodal.html] Chip Huyen Multimodal LLM web research paper.

Medical Visual questions answering : MedVQA
https://paperswithcode.com/task/medical-visual-question-answering

Research Paper to be published on over:
Arxiv
Google-Scholar
ResearchGate

###Evaluation metrics:
F1 scores (precision and recall) , classification accuracy, BLEU (bilingual evaluation understudy) , ROUGE (Recall oriented understudy for gisted evaluation) , MAUVE (https://krishnap25.github.io/mauve/) , ANLS (docvqa -> Average normalized levenshtein similarity), MAP (docvqa -> Mean Average Precision), AUC (area under curve), Confusion matrix


Potential popular conferences:
IEEE
ICVPR
NeurIPS (important)

Helpful Platforms:
Huggingface, Medium (for documentation) / Hashnode / mem.ai / Notion, Github Projects
Todoist (for tracking progress)

